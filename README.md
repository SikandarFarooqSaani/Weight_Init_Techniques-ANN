# üß† Weight Initialization Heuristic Techniques

> **Note:** This README was generated by a custom prompt AI using specific project details provided by the developer to ensure ease of understanding for all readers.

---

## üìå Project Goal
In neural networks, we cannot simply initialize weights with zero or random high/low values, as this hinders training. This notebook explores **heuristic approaches** to solve this:
1. **Glorot Initialization** (for Tanh/Sigmoid activation)
2. **He Initialization** (for ReLU activation)

---

## üõ†Ô∏è Step 1: Data Preparation
* **Libraries:** Imported necessary Python libraries.
* **Dataset:** Classification dataset sourced from Kaggle.
* **Features:** $X$ and $Y$ data points for binary classification.

[<img width="559" height="413" alt="glo1" src="https://github.com/user-attachments/assets/e7e827b8-f79f-403d-8426-e183fd279280" />

---

## üß™ Step 2: The Tanh/Sigmoid Model (Glorot)

### Architecture
* **Input Layer:** 10 nodes
* **Hidden Layers:** 3 layers (10 nodes each) with **Tanh** activation.
* **Output Layer:** 1 node with **Sigmoid** activation.
* **Parameters:** 371 trainable parameters.

### The Manual Initialization Process
1.  **Fetch Weights:** We used `get_weights()` to see the default state.
2.  **Apply Glorot Heuristic:** We calculated new weights using the Glorot formula:
    $$\text{Random Number} \times \sqrt{\frac{1}{\text{No. of Inputs}}}$$
3.  **Set Weights:** We explicitly set these calculated weights into the model.

### Training & Results
* **Training:** Compiled and ran for **100 epochs**.
* **Observation:** Accuracy increased and Loss decreased steadily.
* **Verification:** We ran `get_weights()` again to confirm they changed from the initial state (training happened).

### Analysis
<img width="546" height="413" alt="glo2" src="https://github.com/user-attachments/assets/9ee78962-e016-43ad-bdc5-c79b2dfb4b06" />

* *Observation:* The decision boundary shows a slight underfit.

<img width="567" height="432" alt="glo3" src="https://github.com/user-attachments/assets/fb84cb27-3f13-4cc4-9824-863cc57e5e84" />

* *Observation:* One distribution is perfectly normal, while the $Y$ distribution is "kind of" normally distributed. This indicates the initialization worked reasonably well.

---

## üöÄ Step 3: The ReLU Model (He Initialization)

### Architecture & Setup
* **Model:** Same structure as above (10 node inputs, 3 hidden layers).
* **Change:** Switched activation to **ReLU**.
* **Initializer:** Used the Keras built-in `he_normal` (He Initialization).

### Training & Results
* **Training:** Trained for **100 epochs**.
* **Observation:** The accuracy increased **more** and loss decreased **more** compared to the Tanh/Glorot model.

### Final Analysis
<img width="546" height="413" alt="glo4" src="https://github.com/user-attachments/assets/e0b5e967-f238-4838-9a6d-b63b50f75280" />

* *Observation:* The decision boundary is excellent. It is not overfit and not underfit‚Äîit captures the data structure well.

---

## ‚úÖ Conclusion
This experiment proves that matching the initialization technique to the activation function is crucial:
* **Glorot** works for Tanh/Sigmoid.
* **He Normal** works best for ReLU (providing superior results in this case).
